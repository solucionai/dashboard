# -*- coding: utf-8 -*-
"""estrutura.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U47R0K0SnSKWftzu6sydvopPfxN1pxlx

# Puxando os dados do Analytics
"""

# !pip install google-analytics-data
# !pip install dash dash-bootstrap-components plotly flask

from google.analytics.data_v1beta import BetaAnalyticsDataClient, RunReportRequest, DateRange, Dimension, Metric
import requests
from datetime import datetime, timedelta
from flask import Flask
import dash
from dash import dcc, html, Input, Output
import dash_bootstrap_components as dbc
import plotly.express as px
import pandas as pd

"""# Puxando os dados da API do Pipedrive

"""

# Defina o seu API Token do Pipedrive
API_TOKEN = 'bbdd39fba4dab68ac0c03f4a629680f7429478ff'
BASE_URL = 'https://api.pipedrive.com/v1/'

# Função para extrair todos os dados de negócios utilizando paginação
def fetch_all_pipedrive_deals():
    all_deals = []
    start = 0
    limit = 100  # Número de registros por página

    while True:
        url = f'{BASE_URL}deals?start={start}&limit={limit}&api_token={API_TOKEN}'
        response = requests.get(url)

        if response.status_code == 200:
            deals = response.json().get('data', [])
            if not deals:
                break  # Encerra o loop se não houver mais dados

            all_deals.extend(deals)  # Adiciona os registros atuais à lista geral
            start += limit  # Atualiza o offset para a próxima página
        else:
            print(f"Erro ao extrair dados do Pipedrive: {response.status_code}")
            break

    # Converte todos os negócios extraídos em um DataFrame
    df_all_deals = pd.DataFrame(all_deals)
    print(f"Negócios do Pipedrive extraídos com sucesso. Total de registros: {len(df_all_deals)}")
    return df_all_deals

# Extraindo todos os dados de negócios
df_pipedrive = fetch_all_pipedrive_deals()

# # Verificar quantos números (ou ocorrências) existem em cada valor da coluna 'stage_id'
# stage_counts = df_final_limpo['Etapa_Final'].value_counts()

# # Exibir o resultado
# print(stage_counts)

# df_pipedrive

# # Verificar quantos números (ou ocorrências) existem em cada valor da coluna 'stage_id'
# stage_counts = df_final_limpo['PROBLEMA'].value_counts()

# # Exibir o resultado
# print(stage_counts)

# # Selecionar uma linha aleatória
# random_row = df_pipedrive.sample(n=1)  # Seleciona uma linha aleatória

# # Exibir os valores da linha selecionada
# print("Exemplo de uma linha aleatória:")
# print(random_row)

# # Exibir um exemplo de cada coluna
# print("\nExemplo de cada coluna da linha aleatória:")
# for column in random_row.columns:
#     print(f"{column}: {random_row[column].values[0]}")

# # Calcular a porcentagem de cada valor na coluna 'owner_name'
# percentages = df_pipedrive['owner_name'].value_counts(normalize=True) * 100

# # Exibir as porcentagens
# print(percentages)

# import pandas as pd

# # Supondo que o seu DataFrame se chama df e possui a coluna 'lost_reason'
# # Exemplo: df = pd.read_csv('seu_arquivo.csv')

# # Contar a frequência de cada valor na coluna 'lost_reason'
# lost_reason_counts = df_final_limpo['lost_reason'].value_counts()

# # Calcular a porcentagem de cada valor
# lost_reason_percentages = lost_reason_counts / lost_reason_counts.sum() * 100

# # Exibir os resultados
# print(lost_reason_percentages)

"""# Puxando as etiquetas"""

# Carregar o arquivo etiquetas.xlsx
df_etiquetas = pd.read_excel('etiquetas.xlsx')

# Substituir valores nulos por 0
df_etiquetas = df_etiquetas.fillna(0)

# Salvar como CSV
df_etiquetas.to_csv('etiquetas.csv', index=False)
print("Arquivo etiquetas.xlsx convertido para etiquetas.csv com sucesso e valores nulos substituídos por 0.")

# Carregar o arquivo etiquetas.xlsx
df_etiquetas = pd.read_excel('etiquetas.xlsx')

# Substituir valores nulos e valores não numéricos por 0
df_etiquetas = df_etiquetas.apply(pd.to_numeric, errors='coerce').fillna(0)

# Converter colunas numéricas de float para int
df_etiquetas = df_etiquetas.astype(int)

# Renomear a coluna 'telefone' para 'numero_wpp'
df_etiquetas = df_etiquetas.rename(columns={'Telefone': 'numero_wpp'})

# Adicionar um '+' antes de todos os valores na coluna 'numero_wpp'
df_etiquetas['numero_wpp'] = df_etiquetas['numero_wpp'].apply(lambda x: f'+{x}')

# Salvar como CSV
df_etiquetas.to_csv('etiquetas.csv', index=False)
print("Arquivo etiquetas.xlsx convertido para etiquetas.csv com sucesso, valores nulos substituídos por 0, colunas numéricas convertidas para int, e coluna 'telefone' renomeada para 'numero_wpp'.")

# df_etiquetas

# df_etiquetas.dtypes

"""# Puxando os dados do endpoint"""

# Função para buscar todos os dados do endpoint
def fetch_all_data(url):
    try:
        response = requests.get(url)
        if response.status_code == 200:
            data = response.json()
            return pd.DataFrame(data)  # Converte os dados em um DataFrame
        else:
            print(f"Error: {response.status_code}, Detail: {response.json().get('detail', 'No detail provided')}")
    except requests.RequestException as e:
        print(f"Failed to make the request: {e}")
        return pd.DataFrame()

# URL do endpoint
url = "https://web-production-c353.up.railway.app/retrieve_all"

# Chamando a função e obtendo os dados
df_endpoint = fetch_all_data(url)

# Verificação básica dos dados
if df_endpoint.empty:
    print("Os dados não foram carregados corretamente. Verifique o endpoint ou o formato dos dados.")
    exit()

# Função para calcular o tempo em cada etapa com base nas colunas de timestamp
def calculate_stage_duration(row):
    created_at = pd.to_datetime(row['created_at'], errors='coerce')
    last_modified = pd.to_datetime(row['last_modified'], errors='coerce')
    if pd.isnull(created_at) or pd.isnull(last_modified):
        return 0  # Retorna 0 se houver erro de conversão
    duration = (last_modified - created_at).total_seconds() / 60  # Duração em minutos
    return duration

# Função para extrair a última etapa (flag) alcançada na jornada
def extract_final_stage(raw_data):
    try:
        flags = {
            'voo': int(raw_data.get('FLAG_VOO_JORNADA', 0)),
            'negativacao': int(raw_data.get('FLAG_NEGATIVACAO_JORNADA', 0)),
            'telefonia': int(raw_data.get('FLAG_SERV_TELEF_JORNADA', 0)),
            'bancario': int(raw_data.get('FLAG_SERV_BANCARIO_JORNADA', 0)),
            'compra_online': int(raw_data.get('FLAG_COMPRA_ONLINE_JORNADA', 0)),
            'outros': int(raw_data.get('FLAG_OUTROS_JORNADA', 0)),
            'hospedagem': int(raw_data.get('FLAG_HOSPEDAGEM_JORNADA', 0))
        }
        final_stage = max(flags.values())  # Encontra a maior flag para determinar a etapa final
        return final_stage
    except Exception as e:
        print(f"Erro ao extrair etapa final: {e}")
        return 0

# Aplicar as funções ao dataframe
df_endpoint['created_at'] = pd.to_datetime(df_endpoint['created_at'], errors='coerce')
df_endpoint['Tempo_na_Etapa'] = df_endpoint.apply(calculate_stage_duration, axis=1)
df_endpoint['Etapa_Final'] = df_endpoint['RAW_DATA'].apply(extract_final_stage)
# Substituir valores NaN por 0 em todas as colunas do df_endpoint
df_endpoint = df_endpoint.fillna(0)

# Lista de colunas de interesse
columns = [
    'FLAG_HOSPEDAGEM_JORNADA',
    'FLAG_OUTROS_JORNADA',
    'FLAG_VOO_JORNADA',
    'FLAG_COMPRA_ONLINE_JORNADA',
    'FLAG_SERV_BANCARIO_JORNADA',
    'FLAG_NEGATIVACAO_JORNADA',
    'FLAG_SERV_TELEF_JORNADA'
]

# Converter as colunas para numérico, ignorando erros e preenchendo com NaN para valores não convertíveis
for col in columns:
    df_endpoint[col] = pd.to_numeric(df_endpoint[col], errors='coerce')

# Calcular o valor máximo para cada coluna
max_values = {col: df_endpoint[col].max() for col in columns}

# Determinar o valor máximo global entre todas as flags
max_global = max(max_values.values())

print("Valores máximos de cada coluna:")
for col, max_val in max_values.items():
    print(f"{col}: {max_val}")

print(f"Valor máximo global: {max_global}")

# Criar a coluna FLAG_FINAL, onde 1 indica que a pessoa chegou ao fim e 0 indica que não
df_endpoint['FLAG_FINAL'] = df_endpoint[columns].max(axis=1).apply(lambda x: 1 if x == max_global else 0)

# Exibir as primeiras linhas para ver o resultado
print(df_endpoint[['FLAG_FINAL'] + columns].head())

# df_endpoint.head(30)

# # Contar o número total de registros
# total_registros = len(df_endpoint)

# # Contar o número de registros que chegaram ao final (FLAG_FINAL == 1)
# chegaram_ao_final = df_endpoint[df_endpoint['FLAG_FINAL'] == 1].shape[0]

# # Calcular a porcentagem
# percentual_chegaram_ao_final = (chegaram_ao_final / total_registros) * 100

# # Mostrar o resultado
# print(f"Porcentagem de pessoas que chegaram à etapa final: {percentual_chegaram_ao_final:.2f}%")

# # Selecionar uma linha aleatória
# random_row = df_endpoint.sample(n=1)  # Seleciona uma linha aleatória

# # Exibir os valores da linha selecionada
# print("Exemplo de uma linha aleatória:")
# print(random_row)

# # Exibir um exemplo de cada coluna
# print("\nExemplo de cada coluna da linha aleatória:")
# for column in random_row.columns:
#     print(f"{column}: {random_row[column].values[0]}")

# df_pipedrive

# import pandas as pd

# def exibir_valores_unicos(df, df_name):
#     """
#     Exibe os valores únicos de cada coluna de um DataFrame, lidando com colunas de tipo dict.

#     Parâmetros:
#     - df: DataFrame do qual extrair os valores únicos.
#     - df_name: Nome do DataFrame para identificação nos outputs.
#     """
#     print(f"Valores únicos em cada coluna do DataFrame: {df_name}")
#     for column in df.columns:
#         try:
#             # Converter dicionários para strings para evitar erros de tipo
#             if df[column].apply(lambda x: isinstance(x, dict)).any():
#                 unique_values = df[column].apply(lambda x: str(x) if isinstance(x, dict) else x).unique()
#             else:
#                 unique_values = df[column].unique()

#             print(f"\nColuna: {column}")
#             print(f"Quantidade de valores únicos: {len(unique_values)}")
#             print(f"Valores únicos: {unique_values[:100]}")  # Exibe apenas os primeiros 10 valores para não sobrecarregar
#             print("-" * 50)
#         except Exception as e:
#             print(f"Erro ao processar a coluna: {column}, Erro: {e}")

# # Aplicando a função para cada DataFrame
# exibir_valores_unicos(df_pipedrive, "df_pipedrive")
# exibir_valores_unicos(df_endpoint, "df_endpoint")
# exibir_valores_unicos(df_analytics, "df_analytics")



"""# Unificação e tratamento de dados"""

# Duplicar os DataFrames
df_endpoint_dup = df_endpoint
df_pipedrive_dup = df_pipedrive
df_etiquetas_dup = df_etiquetas

# # Duplicar os DataFrames
# df_endpoint_dup = df_endpoint.copy()
# df_pipedrive_dup = df_pipedrive.copy()
# df_etiquetas_dup = df_etiquetas.copy()

# Quebrar a coluna 'title' no df_pipedrive_dup em três colunas
df_pipedrive_dup[['id2', 'Problema2', 'numero_wpp']] = df_pipedrive_dup['title'].str.extract(r'(\d+)\s*-\s*(.*?)\s*-\s*(\+\d+)')

# Remover a coluna original 'title' depois de extrair os valores
df_pipedrive_dup.drop('title', axis=1, inplace=True)

# # Conferir se a operação foi realizada corretamente
# print(df_pipedrive_dup.head())

# Supondo que você já tenha df_endpoint_dup e df_pipedrive_dup

# Passo 1: Remover duplicatas no numero_wpp para evitar dados inconsistentes
df_endpoint_dup_clean = df_endpoint_dup.drop_duplicates(subset='numero_wpp')
df_pipedrive_dup_clean = df_pipedrive_dup.drop_duplicates(subset='numero_wpp')

# Passo 2: Realizar o merge com base no numero_wpp (inner join para manter apenas os números em comum)
df_merged = pd.merge(df_pipedrive_dup_clean, df_endpoint_dup_clean, on='numero_wpp', how='inner', suffixes=('_pipedrive', '_endpoint'))

# Passo 3: Reorganizar as colunas para que 'numero_wpp' seja a primeira
cols = ['numero_wpp'] + [col for col in df_merged.columns if col != 'numero_wpp']
df_merged = df_merged[cols]

# Passo 4: Igualar os valores da coluna 'deal_id' aos valores da coluna 'pipedrive_deal_id'
df_merged['deal_id'] = df_merged['pipedrive_deal_id']

# Exibir o dataframe resultante para verificar a mudança
# df_merged

# # Selecionar uma linha aleatória
# random_row = df_merged.sample(n=1)  # Seleciona uma linha aleatória

# # Exibir os valores da linha selecionada
# print("Exemplo de uma linha aleatória:")
# print(random_row)

# # Exibir um exemplo de cada coluna
# print("\nExemplo de cada coluna da linha aleatória:")
# for column in random_row.columns:
#     print(f"{column}: {random_row[column].values[0]}")

# # Checar as colunas das linhas específicas dos dataframes df_pipedrive e df_endpoint com base no numero_wpp fornecido

# numero_wpp_check = '+5521964324493'

# # Filtrando as linhas do df_pipedrive e df_endpoint onde numero_wpp é igual ao valor fornecido
# df_pipedrive_row = df_pipedrive_dup[df_pipedrive_dup['numero_wpp'] == numero_wpp_check]
# df_endpoint_row = df_endpoint_dup[df_endpoint_dup['numero_wpp'] == numero_wpp_check]

# # Exibindo as linhas filtradas com todos os valores
# print("Valores da linha do df_pipedrive:\n", df_pipedrive_row.to_string(index=False))
# print("Valores da linha do df_endpoint:\n", df_endpoint_row.to_string(index=False))

# df_etiquetas_dup

# Supondo que já temos o df_merged e o df_etiquetas_dup carregados

# 1. Certificar-se de que a coluna 'numero_wpp' está formatada corretamente em ambos os dataframes
df_merged['numero_wpp'] = df_merged['numero_wpp'].astype(str)
df_etiquetas_dup['numero_wpp'] = df_etiquetas_dup['numero_wpp'].astype(str)

# 2. Realizar o merge com base na coluna 'numero_wpp', mantendo apenas os números que já existem no df_merged
df_final = pd.merge(df_merged, df_etiquetas_dup, on='numero_wpp', how='left')

# 3. Exibir o resultado para verificar se a junção foi realizada corretamente
df_final

# Também podemos verificar a quantidade de linhas para garantir que o merge foi feito corretamente:
print(f"Quantidade de pessoas no df_final: {df_final['numero_wpp'].nunique()}")

# Remover todas as linhas onde o valor na coluna 'POSSÍVEL' seja NaN
df_final_limpo = df_final.dropna(subset=['POSSÍVEL'])

# Exibir as primeiras linhas do DataFrame resultante para verificação
# df_final_limpo

# # Selecionar uma linha aleatória
# random_row = df_final_limpo.sample(n=1)  # Seleciona uma linha aleatória

# # Exibir os valores da linha selecionada
# print("Exemplo de uma linha aleatória:")
# print(random_row)

# # Exibir um exemplo de cada coluna
# print("\nExemplo de cada coluna da linha aleatória:")
# for column in random_row.columns:
#     print(f"{column}: {random_row[column].values[0]}")

# def exibir_valores_unicos(df, df_name):
#     """
#     Exibe os valores únicos de cada coluna de um DataFrame, lidando com colunas de tipo dict.

#     Parâmetros:
#     - df: DataFrame do qual extrair os valores únicos.
#     - df_name: Nome do DataFrame para identificação nos outputs.
#     """
#     print(f"Valores únicos em cada coluna do DataFrame: {df_name}")
#     for column in df.columns:
#         try:
#             # Converter dicionários para strings para evitar erros de tipo
#             if df[column].apply(lambda x: isinstance(x, dict)).any():
#                 unique_values = df[column].apply(lambda x: str(x) if isinstance(x, dict) else x).unique()
#             else:
#                 unique_values = df[column].unique()

#             print(f"\nColuna: {column}")
#             print(f"Quantidade de valores únicos: {len(unique_values)}")
#             print(f"Valores únicos: {unique_values[:100]}")  # Exibe apenas os primeiros 10 valores para não sobrecarregar
#             print("-" * 50)
#         except Exception as e:
#             print(f"Erro ao processar a coluna: {column}, Erro: {e}")

# # Aplicando a função para cada DataFrame
# # exibir_valores_unicos(df_final_limpo, "df_final_limpo")

# # Supondo que o DataFrame seja df_final_limpo

# # 1. Filtrar apenas os dados que não sejam da categoria 'Solucionaí'
# df_filtrado = df_final_limpo[df_final_limpo['owner_name'] != 'Solucionaí']

# # 2. Agrupar por 'owner_name' e contar quantos contratos foram assinados
# # Considerando que a coluna 'CONTRATO ASSINADO' contém 1 para assinado e 0 para não assinado
# contratos_assinados_por_atendente = df_filtrado.groupby('owner_name')['CONTRATO ASSINADO'].sum()

# # 3. Exibir o resultado
# print(contratos_assinados_por_atendente)

# import pandas as pd

# # Supondo que você já tenha um DataFrame chamado df com as colunas "CONTRATOS ASSINADOS" e "wpp_number"
# # Filtrando os números de WhatsApp que têm o valor 1 na coluna "CONTRATOS ASSINADOS"
# filtro_contratos_assinados = df_final_limpo[df_final_limpo['CONTRATO ASSINADO'] == 1]

# # Selecionando a coluna com os números de WhatsApp
# numeros_wpp = filtro_contratos_assinados['numero_wpp']

# # Exibindo os números de WhatsApp
# print(numeros_wpp)

# # Supondo que o DataFrame seja df_final_limpo

# # 1. Filtrar apenas os dados que não sejam da categoria 'Solucionaí'
# df_filtrado = df_final_limpo[df_final_limpo['owner_name'] != 'Solucionaí']

# # 2. Agrupar por 'owner_name' e contar quantos contratos foram assinados
# # Considerando que a coluna 'CONTRATO ASSINADO' contém 1 para assinado e 0 para não assinado
# contratos_assinados_por_atendente = df_filtrado.groupby('owner_name')['CONTRATO ASSINADO'].sum()

# # 3. Contar o total de contratos (considerando tanto assinados quanto não assinados)
# total_contratos_por_atendente = df_filtrado.groupby('owner_name')['CONTRATO ASSINADO'].count()

# # 4. Calcular a porcentagem de contratos assinados por atendente
# porcentagem_contratos_assinados = (contratos_assinados_por_atendente / total_contratos_por_atendente) * 100

# # 5. Exibir o resultado
# print(porcentagem_contratos_assinados)

# import pandas as pd

# # Função para consolidar duplicatas
# def consolidate_duplicates(df, subset_columns, aggregation_func='first'):
#     available_columns = [col for col in subset_columns if col in df.columns]

#     if not available_columns:
#         print(f"Nenhuma das colunas {subset_columns} está presente no DataFrame.")
#         return df

#     # Consolidar duplicatas com base nas colunas disponíveis
#     df_consolidated = df.groupby(available_columns).agg(aggregation_func).reset_index()
#     return df_consolidated

# # Resetar índices dos DataFrames para garantir índices únicos
# df_pipedrive = df_pipedrive.reset_index(drop=True)
# df_endpoint = df_endpoint.reset_index(drop=True)
# df_analytics = df_analytics.reset_index(drop=True)

# # Consolidar duplicatas corretamente
# df_pipedrive = consolidate_duplicates(df_pipedrive, subset_columns=['id'])  # Coluna 'id' usada como Lead ID
# df_endpoint = consolidate_duplicates(df_endpoint, subset_columns=['numero_wpp'])  # Coluna 'numero_wpp' usada como chave

# # Para df_analytics, certifique-se de que a coluna correta está sendo usada
# if 'campaign_name' in df_analytics.columns:
#     df_analytics = consolidate_duplicates(df_analytics, subset_columns=['campaign_name'])

# # Seleção e renomeação de colunas
# df_pipedrive = df_pipedrive[['id', 'status', 'value', 'won_time', 'lost_time', 'lost_reason']]
# df_pipedrive = df_pipedrive.rename(columns={
#     'id': 'Lead ID',
#     'status': 'Status',
#     'value': 'Valor do Negócio',
#     'won_time': 'Data Fechamento',
#     'lost_time': 'Data Fechamento',
#     'lost_reason': 'Motivo da Perda'
# })
# df_pipedrive['Fonte'] = 'Pipedrive'

# df_endpoint = df_endpoint[['numero_wpp', 'created_at', 'last_modified', 'Etapa_Final', 'Tempo_na_Etapa']]
# df_endpoint = df_endpoint.rename(columns={
#     'numero_wpp': 'Número WhatsApp',
#     'created_at': 'Data Captura',
#     'last_modified': 'Data Fechamento',
#     'Etapa_Final': 'Etapa Final',
#     'Tempo_na_Etapa': 'Tempo na Etapa (min)'
# })
# df_endpoint['Fonte'] = 'Bot'

# # Renomeando colunas do df_analytics
# if 'campaign_name' in df_analytics.columns:
#     df_analytics = df_analytics.rename(columns={
#         'campaign_name': 'Canal de Aquisição',
#         'advertiserAdCost': 'Custo do Lead'
#     })
#     df_analytics['Fonte'] = 'Analytics'

# # Resetar índices novamente para garantir unicidade
# df_pipedrive = df_pipedrive.reset_index(drop=True)
# df_endpoint = df_endpoint.reset_index(drop=True)
# df_analytics = df_analytics.reset_index(drop=True)

# # Verificar se há duplicatas nos índices novamente
# print("Verificando duplicatas nos índices...")
# print(f"Duplicatas em df_pipedrive: {df_pipedrive.index.duplicated().sum()}")
# print(f"Duplicatas em df_endpoint: {df_endpoint.index.duplicated().sum()}")
# print(f"Duplicatas em df_analytics: {df_analytics.index.duplicated().sum()}")

# # Garantir que não há duplicatas nos índices antes de concatenar
# df_pipedrive = df_pipedrive[~df_pipedrive.index.duplicated()]
# df_endpoint = df_endpoint[~df_endpoint.index.duplicated()]
# df_analytics = df_analytics[~df_analytics.index.duplicated()]

# # Forçar redefinição de índices e conversão de índices para garantir unicidade
# df_pipedrive.index = pd.RangeIndex(start=0, stop=len(df_pipedrive), step=1)
# df_endpoint.index = pd.RangeIndex(start=0, stop=len(df_endpoint), step=1)
# df_analytics.index = pd.RangeIndex(start=0, stop=len(df_analytics), step=1)

# # Concatenar DataFrames
# df_unificado = pd.concat([df_pipedrive, df_endpoint, df_analytics], ignore_index=True, sort=False)

# # Visualizar o DataFrame unificado
# print(df_unificado.head())

# Certificar que estamos trabalhando com uma cópia do DataFrame para evitar o SettingWithCopyWarning
df_final_limpo = df_final_limpo.copy()

# Dicionário para mapear os DDDs aos estados
ddd_mapping_estados = {
    61: 'Distrito Federal',
    62: 'Goiás', 64: 'Goiás',
    65: 'Mato Grosso', 66: 'Mato Grosso',
    67: 'Mato Grosso do Sul',
    82: 'Alagoas',
    71: 'Bahia', 73: 'Bahia', 74: 'Bahia', 75: 'Bahia', 77: 'Bahia',
    85: 'Ceará', 88: 'Ceará',
    98: 'Maranhão', 99: 'Maranhão',
    83: 'Paraíba',
    81: 'Pernambuco', 87: 'Pernambuco',
    86: 'Piauí', 89: 'Piauí',
    84: 'Rio Grande do Norte',
    79: 'Sergipe',
    68: 'Acre',
    96: 'Amapá',
    92: 'Amazonas', 97: 'Amazonas',
    91: 'Pará', 93: 'Pará', 94: 'Pará',
    69: 'Rondônia',
    95: 'Roraima',
    63: 'Tocantins',
    27: 'Espírito Santo', 28: 'Espírito Santo',
    31: 'Minas Gerais', 32: 'Minas Gerais', 33: 'Minas Gerais', 34: 'Minas Gerais',
    35: 'Minas Gerais', 37: 'Minas Gerais', 38: 'Minas Gerais',
    21: 'Rio de Janeiro', 22: 'Rio de Janeiro', 24: 'Rio de Janeiro',
    11: 'São Paulo', 12: 'São Paulo', 13: 'São Paulo', 14: 'São Paulo',
    15: 'São Paulo', 16: 'São Paulo', 17: 'São Paulo', 18: 'São Paulo', 19: 'São Paulo',
    41: 'Paraná', 42: 'Paraná', 43: 'Paraná', 44: 'Paraná', 45: 'Paraná', 46: 'Paraná',
    51: 'Rio Grande do Sul', 53: 'Rio Grande do Sul', 54: 'Rio Grande do Sul', 55: 'Rio Grande do Sul',
    47: 'Santa Catarina', 48: 'Santa Catarina', 49: 'Santa Catarina'
}

# Função para garantir que o DDD é numérico e tratar erros
def tratar_ddd(ddd):
    try:
        return int(ddd)
    except ValueError:
        return None

# Tratando os valores de DDD
df_final_limpo['DDD'] = df_final_limpo['DDD'].apply(tratar_ddd)

# Aplicar o mapeamento de DDDs para Estados usando .loc para evitar SettingWithCopyWarning
df_final_limpo.loc[:, 'Estado'] = df_final_limpo['DDD'].map(ddd_mapping_estados)

# Exibindo os primeiros resultados para verificar se a mudança foi aplicada corretamente
print(df_final_limpo[['DDD', 'Estado']].head())

# # Mapeamento de DDD para regiões
# ddd_regioes = {
#     'Centro-Oeste': [61, 62, 64, 65, 66, 67],
#     'Nordeste': [82, 71, 73, 74, 75, 77, 85, 88, 98, 99, 83, 81, 87, 86, 89, 84, 79],
#     'Norte': [68, 96, 92, 97, 91, 93, 94, 69, 95, 63],
#     'Sudeste': [27, 28, 31, 32, 33, 34, 35, 37, 38, 21, 22, 24, 11, 12, 13, 14, 15, 16, 17, 18, 19],
#     'Sul': [41, 42, 43, 44, 45, 46, 51, 53, 54, 55, 47, 48, 49]
# }

# # Função para mapear DDD para região
# def map_ddd_to_regiao(ddd):
#     for regiao, ddds in ddd_regioes.items():
#         if int(ddd) in ddds:
#             return regiao
#     return 'Desconhecido'  # Caso o DDD não esteja mapeado

# # Adiciona a coluna de Região ao DataFrame
# df_final_limpo['Regiao'] = df_final_limpo['DDD'].apply(map_ddd_to_regiao)

# # Dicionário para mapear os DDDs aos estados e regiões
# # Dicionário para mapear os DDDs aos estados
# ddd_mapping_estados = {
#     61: 'Distrito Federal',
#     62: 'Goiás', 64: 'Goiás',
#     65: 'Mato Grosso', 66: 'Mato Grosso',
#     67: 'Mato Grosso do Sul',
#     82: 'Alagoas',
#     71: 'Bahia', 73: 'Bahia', 74: 'Bahia', 75: 'Bahia', 77: 'Bahia',
#     85: 'Ceará', 88: 'Ceará',
#     98: 'Maranhão', 99: 'Maranhão',
#     83: 'Paraíba',
#     81: 'Pernambuco', 87: 'Pernambuco',
#     86: 'Piauí', 89: 'Piauí',
#     84: 'Rio Grande do Norte',
#     79: 'Sergipe',
#     68: 'Acre',
#     96: 'Amapá',
#     92: 'Amazonas', 97: 'Amazonas',
#     91: 'Pará', 93: 'Pará', 94: 'Pará',
#     69: 'Rondônia',
#     95: 'Roraima',
#     63: 'Tocantins',
#     27: 'Espírito Santo', 28: 'Espírito Santo',
#     31: 'Minas Gerais', 32: 'Minas Gerais', 33: 'Minas Gerais', 34: 'Minas Gerais',
#     35: 'Minas Gerais', 37: 'Minas Gerais', 38: 'Minas Gerais',
#     21: 'Rio de Janeiro', 22: 'Rio de Janeiro', 24: 'Rio de Janeiro',
#     11: 'São Paulo', 12: 'São Paulo', 13: 'São Paulo', 14: 'São Paulo',
#     15: 'São Paulo', 16: 'São Paulo', 17: 'São Paulo', 18: 'São Paulo', 19: 'São Paulo',
#     41: 'Paraná', 42: 'Paraná', 43: 'Paraná', 44: 'Paraná', 45: 'Paraná', 46: 'Paraná',
#     51: 'Rio Grande do Sul', 53: 'Rio Grande do Sul', 54: 'Rio Grande do Sul', 55: 'Rio Grande do Sul',
#     47: 'Santa Catarina', 48: 'Santa Catarina', 49: 'Santa Catarina'
# }

# # Aplicando o mapeamento de DDDs para Estados
# df_final_limpo['Estado'] = df_final_limpo['DDD'].map(ddd_mapping_estados)



# # Verifique se o df_final_limpo está carregado
# if 'df_final_limpo' in locals():
#     # Checando os valores únicos na coluna DDD
#     valores_unicos_ddd = df_final_limpo['DDD'].unique()
#     print("Valores únicos da coluna DDD:", valores_unicos_ddd)
# else:
#     print("O DataFrame df_final_limpo não está carregado.")

# Criando o mapeamento correto para renomear os valores de 'stage_id' de números para strings
stage_mapping = {
    1: 'Captados',
    8: 'Em Análise',
    7: 'Recuperação'
}

# Aplicando o mapeamento corretamente usando .loc para evitar o SettingWithCopyWarning
df_final_limpo.loc[:, 'stage_id'] = df_final_limpo['stage_id'].map(stage_mapping)

# # Verifique se o mapeamento foi aplicado corretamente
# print(df_final_limpo['stage_id'].unique())

# print(df_final_limpo['stage_id'].unique())
# # Exemplo de mapeamento ajustado baseado em valores strings (caso os valores de 'stage_id' sejam strings)
# stage_mapping = {
#     'Captados': 1,
#     'Em Análise': 8,
#     'Recuperação': 7
# }

# # Aplicando o mapeamento para transformar valores de 'stage_id'
# df_final_limpo['stage_id'] = df_final_limpo['stage_id'].map(stage_mapping)

# # Verifique se o mapeamento foi aplicado corretamente
# print(df_final_limpo['stage_id'].unique())

# # Criando o mapeamento para renomear os valores de 'stage_id'
# stage_mapping = {
#     8: 'Em Análise',
#     7: 'Recuperação',
#     1: 'Captados'
# }

# # Aplicando o mapeamento com o uso de .loc para evitar o warning
# df_final_limpo.loc[:, 'stage_id'] = df_final_limpo['stage_id'].map(stage_mapping)

# # Exibindo os valores únicos para verificar se a mudança foi aplicada corretamente
# # print(df_final_limpo['stage_id'].unique())

# import pandas as pd

# # Remover a limitação de colunas abreviadas
# pd.set_option('display.max_columns', None)  # Agora ele mostrará todas as colunas
# pd.set_option('display.max_rows', None)     # Mostra todas as linhas (se necessário)
# pd.set_option('display.max_colwidth', None) # Mostra todo o conteúdo da célula

# # Calcular a porcentagem de valores nulos em cada coluna
# percentual_nulos = (df_final_limpo.isnull().sum() / len(df)) * 100

# # Exibir o resultado
# print(percentual_nulos)

# Identificar as colunas que possuem 100% de valores nulos
cols_to_drop = df_final_limpo.columns[df_final_limpo.isnull().sum() == len(df_final_limpo)]

# Dropar as colunas identificadas
df_final_limpo = df_final_limpo.drop(columns=cols_to_drop)

# Substituir todos os valores nulos por 0, exceto na coluna 'lost_reason'
df_final_limpo = df_final_limpo.apply(lambda col: col.fillna(0) if col.name != 'lost_reason' else col)

import dash
from dash import dcc, html
import dash_bootstrap_components as dbc
import plotly.express as px
import pandas as pd
from dash.dependencies import Input, Output

# Utilizando a base de dados df_final_limpo já carregada
df = df_final_limpo

# Inicializar o aplicativo Dash
app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP], suppress_callback_exceptions=True)

# Estilos personalizados
SIDEBAR_STYLE = {
    "position": "fixed",
    "top": 0,
    "left": 0,
    "bottom": 0,
    "width": "20%",
    "padding": "20px",
    "background-color": "#343a40",  # Cor da barra lateral (cinza escuro)
}

CONTENT_STYLE = {
    "margin-left": "20%",
    "padding": "20px",
    "background-color": "#f8f9fa",  # Cor da parte central/direita (cinza claro)
    "text-align": "center"  # Centralizar o texto
}

# Layout da barra lateral
sidebar = html.Div(
    [
        html.H2("Dashboard", className="display-6", style={"color": "white"}),
        html.Hr(),
        html.P("Solucionaí", className="lead", style={"color": "white"}),
        dbc.Nav(
            [
                dbc.NavLink("Home", href="/home", id="home-button", style={"color": "white"}),
                dbc.NavLink("Leads", href="/leads", id="leads-button", style={"color": "white"}),
                dbc.NavLink("Análise detalhada de leads", href="/analise-leads", id="analise-leads-button", style={"color": "white"}),
                dbc.NavLink("Atendentes", href="/atendentes", id="atendentes-button", style={"color": "white"}),
            ],
            vertical=True,
            pills=True,
        ),
    ],
    style=SIDEBAR_STYLE,
)

# Layout da parte central/direita
content = html.Div(
    [
        html.H2("Dashboard de Dados Solucionaí", className="display-4"),
        html.Hr(),
        html.Div(id="page-content"),
    ],
    style=CONTENT_STYLE,
)

# Layout principal combinando a barra lateral e o conteúdo
app.layout = html.Div([sidebar, content])

# Callback para atualizar o conteúdo da página
@app.callback(
    Output("page-content", "children"),
    [Input("home-button", "n_clicks"),
     Input("leads-button", "n_clicks"),
     Input("analise-leads-button", "n_clicks"),
     Input("atendentes-button", "n_clicks")]
)
def display_page(n_home, n_leads, n_analise, n_atendentes):
    ctx = dash.callback_context  # Verificar qual botão foi clicado
    if not ctx.triggered:
        return html.H3("Home, seja bem-vindo ao Dashboard de Dados da Solucionaí!")
    else:
        button_id = ctx.triggered[0]['prop_id'].split('.')[0]
        if button_id == "home-button":
            return html.H3("Home, seja bem-vindo ao Dashboard de Dados da Solucionaí!")
        elif button_id == "leads-button":
            try:
                # Dropdown para selecionar o problema
                dropdown_problema = dcc.Dropdown(
                    id='problema-dropdown',
                    options=[{'label': problema, 'value': problema} for problema in df['PROBLEMA'].unique()],
                    value=df['PROBLEMA'].unique()[0] if not df.empty else None,
                    clearable=False,
                    style={'width': '50%', 'margin-bottom': '20px'}
                )

                # Gráfico 1: Total de Leads Cadastrados na Base por Período de Tempo
                leads_by_date = df.groupby(df['created_at'].dt.date).size()
                fig_total_leads = px.line(x=leads_by_date.index, y=leads_by_date.values, title="Total de Leads por Período",
                                          labels={'x': 'Data', 'y': 'Número de Leads'})

                # 2. Leads Cadastrados na Base por Tipo de Problema
                leads_by_problem = df['PROBLEMA'].value_counts()
                fig_leads_problema = px.bar(x=leads_by_problem.index, y=leads_by_problem.values, title="Leads por Tipo de Problema",
                                            labels={'x': 'Tipo de Problema', 'y': 'Contagem de Leads'})

                # 3. Leads Cadastrados por Região ou DDD
                leads_by_ddd = df['Estado'].value_counts()
                leads_by_ddd = leads_by_ddd[leads_by_ddd > 1]  # Filtrar para exibir apenas os valores maiores que 1
                fig_leads_ddd = px.bar(x=leads_by_ddd.index, y=leads_by_ddd.values, title="Leads por Região",
                                       labels={'x': 'Estado', 'y': 'Contagem de Leads'})

                # 4. Leads que Responderam o Fluxo até o Final
                completed_flow = df[df['COMPLETOU_O_FLUXO'] == 1].shape[0]
                fig_fluxo_completo = px.pie(values=[completed_flow, df.shape[0] - completed_flow],
                                            names=['Completaram', 'Não Completaram'],
                                            title="Leads que Completaram o Fluxo até o Final")

                return html.Div([
                    dropdown_problema,  # Dropdown para selecionar o problema
                    dcc.Graph(figure=fig_total_leads),  # Gráfico 1: Total de Leads
                    html.Div(id="leads-info", style={'text-align': 'center', 'margin': '20px 0'}),  # Contador de leads captados
                    dcc.Graph(figure=fig_leads_problema),  # Gráfico 2: Leads por Tipo de Problema
                    dcc.Graph(figure=fig_leads_ddd),  # Gráfico 3: Leads por Região ou DDD
                    dcc.Graph(figure=fig_fluxo_completo),  # Gráfico 4: Leads que completaram o fluxo
                    dcc.Graph(id="tempo-por-lead-wpp-graph")  # Gráfico de Tempo por Lead (número de WhatsApp)
                ])
            except Exception as e:
                return html.Div(f"Erro ao carregar os dados: {str(e)}")

        elif button_id == "analise-leads-button":
            # Gráficos da análise detalhada de leads
            contratos_by_date = df[df['CONTRATO ASSINADO'] == 1].groupby(df['created_at'].dt.date).size()
            fig_contratos_tempo = px.line(x=contratos_by_date.index, y=contratos_by_date.values, title="Contratos Fechados por Período",
                                          labels={'x': 'Data', 'y': 'Número de Contratos Fechados'})

            leads_nao_prosseguiram = df[(df['ClienteDesistiu'] == 1) | (df['SEM RESPOSTA'] == 1)].shape[0]
            fig_leads_nao_prosseguiram = px.pie(values=[leads_nao_prosseguiram, df.shape[0] - leads_nao_prosseguiram],
                                                names=['Não Prosseguiram', 'Prosseguiram'],
                                                title="Leads que Não Prosseguiram após o Primeiro Contato")

            motivo_perda = df[df['NÃO ELEGÍVEL'] == 1]['lost_reason'].value_counts()
            fig_motivos_perda_leads = px.bar(x=motivo_perda.index, y=motivo_perda.values, title="Motivo de Perda dos Leads Não Elegíveis",
                                             labels={'x': 'Motivo', 'y': 'Número de Leads Não Elegíveis'})

            return html.Div([
                dcc.Graph(figure=fig_contratos_tempo),  # Gráfico 5: Contratos fechados por tempo
                dcc.Graph(figure=fig_leads_nao_prosseguiram),  # Gráfico 7: Leads que Não Prosseguiram
                dcc.Graph(figure=fig_motivos_perda_leads)  # Gráfico 8: Motivo de Perda dos Leads Não Elegíveis
            ])
        elif button_id == "atendentes-button":
            # Filtrar os dados para remover 'Solucionaí'
            df_filtered_atendentes = df[df['owner_name'] != 'Solucionaí']

            atendimentos_por_dia = df_filtered_atendentes.groupby([df_filtered_atendentes['created_at'].dt.date, 'owner_name']).size().unstack().fillna(0)
            fig_atendimentos_por_dia = px.line(atendimentos_por_dia, title="Atendimentos por Dia por Atendente",
                                               labels={'x': 'Data', 'y': 'Número de Atendimentos'})

            interacoes_por_lead = df_filtered_atendentes.groupby('owner_name').size()
            fig_interacoes_por_lead = px.bar(x=interacoes_por_lead.index, y=interacoes_por_lead.values, title="Total de Interações com Leads por Atendente",
                                             labels={'x': 'Atendente', 'y': 'Número de Leads'})

            contratos_por_atendente = df_filtered_atendentes[df_filtered_atendentes['CONTRATO ASSINADO'] == 1].groupby('owner_name').size()
            fig_contratos_por_atendente = px.bar(x=contratos_por_atendente.index, y=contratos_por_atendente.values, title="Contratos Fechados por Atendente",
                                                 labels={'x': 'Atendente', 'y': 'Número de Contratos Fechados'})

            return html.Div([
                dcc.Graph(figure=fig_atendimentos_por_dia),  # Gráfico 9: Atendimentos por Dia por Atendente
                dcc.Graph(figure=fig_interacoes_por_lead),  # Gráfico 11: Interações por Lead por Atendente
                dcc.Graph(figure=fig_contratos_por_atendente)  # Gráfico 12: Contratos Fechados por Atendente
            ])

# Callback para o gráfico de tempo na etapa por número de WhatsApp com filtro por problema
@app.callback(
    [Output('tempo-por-lead-wpp-graph', 'figure'),
     Output('leads-info', 'children')],
    [Input('problema-dropdown', 'value')]
)
def update_tempo_por_lead_graph(selected_problema):
    # Filtrar dados pelo problema selecionado
    df_filtered_by_problem = df[df['PROBLEMA'] == selected_problema]

    # Remover valores de tempo na etapa acima de 40 minutos e valores 0
    df_filtered = df_filtered_by_problem[(df_filtered_by_problem['Tempo_na_Etapa'] <= 40) &
                                         (df_filtered_by_problem['Tempo_na_Etapa'] > 0)]

    # Gráfico de barras para tempo por lead (número de WhatsApp)
    fig_tempo_por_lead_wpp = px.bar(
        df_filtered,
        x='numero_wpp',
        y='Tempo_na_Etapa',
        title='Tempo na Etapa por Lead (Filtrado)',
        labels={'Tempo_na_Etapa': 'Tempo na Etapa (minutos)', 'numero_wpp': 'Número WhatsApp'},
        hover_data={'Etapa_Final': True}  # Adiciona a etapa final ao hover
    )

    # Atualizar as métricas de leads
    leads_captados_total = len(df['numero_wpp'].unique())  # Número total de leads únicos
    leads_captados = len(df_filtered_by_problem['numero_wpp'].unique())  # Leads únicos para o problema selecionado
    proporcao_responderam = (leads_captados / leads_captados_total * 100) if leads_captados_total > 0 else 0

    leads_info = [
        html.H4(f"Leads Captados no Problema Selecionado: {leads_captados}"),
        html.H4(f"Total de Leads Captados: {leads_captados_total}"),
        html.H4(f"Proporção de Leads no Problema Selecionado: {proporcao_responderam:.2f}%")
    ]

    return fig_tempo_por_lead_wpp, leads_info

# Rodar o aplicativo
if __name__ == "__main__":
    app.run_server(debug=True)

